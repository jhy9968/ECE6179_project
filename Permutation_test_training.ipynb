{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOBpG7FbLWdO1aJ6QszZxOK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhy9968/ECE6179_project/blob/main/Permutation_test_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "PNTXSrzaqfKB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnlcoHFBaGvj",
        "outputId": "dd1d8951-3712-4720-8b04-a5e37f228009",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pds\n",
        "from tqdm import tqdm\n",
        "from time import sleep\n",
        "from scipy import optimize\n",
        "from datetime import datetime\n",
        "from sklearn import preprocessing\n",
        "from torch.utils.data import Dataset\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from IPython.display import clear_output, display\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "GPU_indx = 0\n",
        "device = torch.device(GPU_indx if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "root_dir = '/content/drive/Shareddrives/Intersection_following_data/'\n",
        "data = np.load(root_dir + 'data.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "Fd42lqAwvVA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model, Dataset Class & Functions"
      ],
      "metadata": {
        "id": "sJ8ObqAKuwMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearModel(torch.nn.Module):\n",
        "\n",
        "  def __init__(self,input_dim=2,output_dim=2):\n",
        "    super(LinearModel,self).__init__()\n",
        "\n",
        "    self.fc1 = torch.nn.Linear(input_dim,output_dim)\n",
        "    # self.log_sigma = torch.nn.Parameter(torch.ones(2,))\n",
        "    self.log_sigma = torch.nn.Linear(input_dim,output_dim)\n",
        "  \n",
        "  def sample(self,x):\n",
        "    dist = self.forward(x)\n",
        "    return dist.sample()\n",
        "\n",
        "  def get_stats(self,x):\n",
        "    dist = self.forward(x)\n",
        "    return dist.loc, dist.covariance_matrix\n",
        "\n",
        "  def forward(self,x):\n",
        "    mu = self.fc1(x)\n",
        "\n",
        "    sigma = torch.exp(self.log_sigma(x))\n",
        "    cov = torch.diag_embed(sigma)\n",
        "\n",
        "    m = MultivariateNormal(mu,cov)\n",
        "    return m"
      ],
      "metadata": {
        "id": "ejWnHSTcwBPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, data, var_names, permutation=False):\n",
        "    # Grab column names\n",
        "    col_arr = data[0].columns.to_numpy()\n",
        "    # Find the baseline input variables indexes and label indexes\n",
        "    mask = np.array(['B' in col for col in col_arr])\n",
        "    X_idx = np.array(range(len(mask)))[mask][:-2]\n",
        "    Y_idx = np.array(range(len(mask)))[mask][-2:]\n",
        "    self.input_names = col_arr[mask][:-2]\n",
        "    self.output_names = col_arr[mask][-2:]\n",
        "    # Find and append external input variables indexes\n",
        "    for name in var_names:\n",
        "      mask = np.array([name in col for col in col_arr])\n",
        "      X_idx = np.append(X_idx, np.array(range(len(mask)))[mask])\n",
        "      self.input_names = np.append(self.input_names, col_arr[mask])\n",
        "    X_idx = np.sort(X_idx.astype('int32'))\n",
        "    # Convert and concatenate all the data into one np array\n",
        "    np_data = pds.concat(data,axis=0,ignore_index=True).to_numpy()\n",
        "\n",
        "    self.X_idx = X_idx\n",
        "    self.Y_idx = Y_idx\n",
        "\n",
        "    self.X = np_data[:,X_idx]\n",
        "    self.Y = np_data[:,Y_idx]\n",
        "    # Randomly shuffle the labels for permutation test\n",
        "    if permutation:\n",
        "      self.Y = np.random.shuffle(self.Y)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = self.X[idx,:]\n",
        "    y = self.Y[idx,:]\n",
        "    return x, y\n",
        "\n",
        "  def get_idxs(self):\n",
        "    return self.X_idx, self.Y_idx\n",
        "  \n",
        "  def in_out_dim(self):\n",
        "    return self.X.shape[1], self.Y.shape[1]\n",
        "\n",
        "  def in_out_names(self):\n",
        "    return self.input_names, self.output_names"
      ],
      "metadata": {
        "id": "FeSuhcCLJU64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(epoch_index, gpu=False):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    running_visual_loss = 0.\n",
        "    last_visual_loss = 0.\n",
        "    \n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = torch.unsqueeze(inputs, dim=1).float()\n",
        "        labels = labels.float()\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "        if gpu:\n",
        "          inputs = inputs.to(device)\n",
        "          outputs = model(inputs)\n",
        "          outputs.loc = outputs.loc.cpu()\n",
        "          outputs.covariance_matrix = outputs.covariance_matrix.cpu()\n",
        "        else:\n",
        "          outputs = model(inputs)\n",
        "\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        visual_loss = visual_loss_fn(torch.squeeze(outputs.loc), labels)\n",
        "        loss.backward()\n",
        "\n",
        "        optimiser.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_visual_loss += visual_loss.item()\n",
        "\n",
        "    last_loss = running_loss / len(train_loader) # loss per batch\n",
        "    last_visual_loss = running_visual_loss / len(train_loader)\n",
        "\n",
        "    return last_loss, last_visual_loss\n",
        "\n",
        "\n",
        "def loss_fn(y_pred, y_true):\n",
        "  y_pred.loc = torch.squeeze(y_pred.loc)\n",
        "  y_pred.covariance_matrix = torch.squeeze(y_pred.covariance_matrix)\n",
        "  return torch.mean(-MultivariateNormal(y_pred.loc, y_pred.covariance_matrix).log_prob(y_true))"
      ],
      "metadata": {
        "id": "BJEKsKKiYoK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline"
      ],
      "metadata": {
        "id": "jHATkbP30-FD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "HfAw682Pu1Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "baseline_set = MyDataset(data, var_names=[], permutation=True)\n",
        "\n",
        "#Use 90% of data for training\n",
        "num_train = int(len(baseline_set)*0.9)\n",
        "\n",
        "#Split data into train/val sets\n",
        "torch.manual_seed(0) #Set torch's random seed so that random split of data is reproducible\n",
        "train_set, val_set = random_split(baseline_set, [num_train, len(baseline_set)-num_train])\n",
        "\n",
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(val_set, batch_size=batch_size)\n",
        "\n",
        "print('Number of training data:', len(train_set))\n",
        "print('Number of validation data:', len(val_set))\n",
        "test_x, test_y = next(iter(train_loader))\n",
        "print('x shape:', test_x.shape, '\\ny shape:', test_y.shape)"
      ],
      "metadata": {
        "id": "qx1atk4JdjDe",
        "outputId": "7ba7d001-4e8e-4386-ab70-136a45e7be4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training data: 34614\n",
            "Number of validation data: 3847\n",
            "x shape: torch.Size([256, 48]) \n",
            "y shape: torch.Size([256, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 4000\n",
        "LEARNING_RATE = 1e-2\n",
        "best_vloss = 1_000_000\n",
        "\n",
        "if not os.path.isdir(root_dir + \"Training/runs/\"):\n",
        "  os.makedirs(root_dir + \"Training/runs/\")\n",
        "if not os.path.isdir(root_dir + \"Training/models/\"):\n",
        "  os.makedirs(root_dir + \"Training/models/\")\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "writer = SummaryWriter(root_dir + 'Training/runs/permutation_baseline_{}_epochs_{}_lr_{}'.format(timestamp, EPOCHS, LEARNING_RATE))\n",
        "epoch_number = 0\n",
        "\n",
        "input_dim, output_dim = baseline_set.in_out_dim()\n",
        "model = LinearModel(input_dim=input_dim,output_dim=output_dim)\n",
        "if torch.cuda.is_available():\n",
        "  model = model.to(device)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "visual_loss_fn = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "ZkZMphYVYdGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=/content/drive/Shareddrives/Intersection_following_data/Training/runs"
      ],
      "metadata": {
        "id": "cSxxL6K3lHKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_avg_loss = 1e20\n",
        "min_avg_visual_loss = 1e20\n",
        "min_avg_vloss = 1e20\n",
        "min_avg_visual_vloss = 1e20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    # Training\n",
        "    model.train(True)\n",
        "    avg_loss, avg_visual_loss = train_one_epoch(epoch_number, gpu=torch.cuda.is_available())\n",
        "    if avg_loss < min_avg_loss:\n",
        "      min_avg_loss = avg_loss\n",
        "    if avg_visual_loss < min_avg_visual_loss:\n",
        "      min_avg_visual_loss = avg_visual_loss\n",
        "\n",
        "    # Validation\n",
        "    model.train(False)\n",
        "    running_vloss = 0.0\n",
        "    running_visual_vloss = 0.0\n",
        "    running_vacc = 0.0\n",
        "    with torch.no_grad():\n",
        "      for i, vdata in enumerate(valid_loader):\n",
        "          vinputs, vlabels = vdata\n",
        "          vinputs = torch.unsqueeze(vinputs, dim=1).float()\n",
        "          vlabels = vlabels.float()\n",
        "          if torch.cuda.is_available():\n",
        "            voutputs = model(vinputs.to(device))\n",
        "            voutputs.loc = voutputs.loc.cpu()\n",
        "            voutputs.covariance_matrix = voutputs.covariance_matrix.cpu()\n",
        "          else:\n",
        "            voutputs = model(vinputs)\n",
        "          vloss = loss_fn(voutputs, vlabels)\n",
        "          visual_vloss = visual_loss_fn(torch.squeeze(voutputs.loc), vlabels)\n",
        "          running_vloss += vloss\n",
        "          running_visual_vloss += visual_vloss\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    avg_visual_vloss = running_visual_vloss / (i + 1)\n",
        "    if avg_vloss < min_avg_vloss:\n",
        "      min_avg_vloss = avg_vloss\n",
        "    if avg_visual_vloss < min_avg_visual_vloss:\n",
        "      min_avg_visual_vloss = avg_visual_vloss\n",
        "\n",
        "    # Print loss\n",
        "    if epoch_number % int(EPOCHS/10) == int(EPOCHS/10)-1:\n",
        "      print('EPOCH {}: LOSS train {} min. train {} valid {}'.format(epoch_number+1, avg_loss, min_avg_loss, avg_vloss))\n",
        "    \n",
        "    # Log loss\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss},\n",
        "                    epoch_number + 1)\n",
        "    writer.add_scalars('Training vs. Validation MSE Loss',\n",
        "                    { 'Training' : avg_visual_loss, 'Validation' : avg_visual_vloss},\n",
        "                    epoch_number + 1)\n",
        "    writer.add_scalars('Training vs. Validation Min Loss',\n",
        "                    { 'Training' : min_avg_loss, 'Validation' : min_avg_vloss},\n",
        "                    epoch_number + 1)\n",
        "    writer.add_scalars('Training vs. Validation Min MSE Loss',\n",
        "                    { 'Training' : min_avg_visual_loss, 'Validation' : min_avg_visual_vloss},\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = root_dir + 'Training/models/permutation_best_baseline{}'.format(timestamp)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1"
      ],
      "metadata": {
        "id": "jMGd18YzYfuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "hNBMhfbou_5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim, output_dim = baseline_set.in_out_dim()\n",
        "best_model = LinearModel(input_dim=input_dim,output_dim=output_dim)\n",
        "# timestamp = \"20221013223324\"\n",
        "best_model.load_state_dict(torch.load(root_dir + 'Training/models/permutation_best_baseline{}'.format(timestamp)))"
      ],
      "metadata": {
        "id": "Yl1EwRMj6h4s",
        "outputId": "c4e3c65b-8f43-41aa-db19-eea3ff0c8b99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_names, out_names = baseline_set.get_idxs()\n",
        "root_dir='/content/drive/Shareddrives/Intersection_following_data/'\n",
        "\n",
        "row = 10\n",
        "col = 4\n",
        "fig, axs = plt.subplots(row,col, figsize=(col*5,row*5))\n",
        "\n",
        "entropy_list_baseline = []\n",
        "\n",
        "for data_id in range(row):\n",
        "\n",
        "  test_x = torch.from_numpy((data[data_id].to_numpy()[:,in_names])).float()\n",
        "  test_y = torch.from_numpy((data[data_id].to_numpy()[:,out_names])).float()\n",
        "  # test_x = torch.unsqueeze(test_x, dim=1).float()\n",
        "  pred_y = best_model(test_x)\n",
        "\n",
        "  test_y = torch.squeeze(test_y).detach().numpy()\n",
        "  pred_y.loc = torch.squeeze(pred_y.loc)\n",
        "  pred_y.covariance_matrix = torch.squeeze(pred_y.covariance_matrix)\n",
        "  pred_y_loc = pred_y.loc.detach().numpy()\n",
        "\n",
        "  # SEs = np.square(np.linalg.norm((test_y-pred_y_loc), axis=1))\n",
        "\n",
        "  # samples = []\n",
        "  # for i in range(pred_y.loc.shape[0]):\n",
        "  #   multinorm = MultivariateNormal(loc=pred_y.loc[i], covariance_matrix=pred_y.covariance_matrix[i])\n",
        "  #   for j in range(10):\n",
        "  #     samples.append(multinorm.sample().detach().numpy())\n",
        "  # samples = np.stack(samples)\n",
        "\n",
        "  entropy = []\n",
        "  for i in range(pred_y.loc.shape[0]):\n",
        "    multinorm = MultivariateNormal(loc=pred_y.loc[i], covariance_matrix=pred_y.covariance_matrix[i])\n",
        "    entropy.append(multinorm.entropy().detach().numpy())\n",
        "  entropy = np.stack(entropy)\n",
        "  entropy_list_baseline.append(entropy)\n",
        "\n",
        "#   axs[data_id, 0].plot(test_y[:,0], test_y[:,1], 'blue', linewidth=2, label='True trajectory')\n",
        "#   axs[data_id, 0].plot(pred_y_loc[:,0], pred_y_loc[:,1], 'red', linewidth=2, label='Prediction')\n",
        "#   axs[data_id, 0].set_aspect('equal')\n",
        "\n",
        "#   axs[data_id, 1].scatter(samples[:,0], samples[:,1], s=5, c='gray', alpha=0.2, label='Samples')\n",
        "#   axs[data_id, 1].plot(test_y[:,0], test_y[:,1], 'blue', linewidth=2, label='True trajectory')\n",
        "#   axs[data_id, 1].plot(pred_y_loc[:,0], pred_y_loc[:,1], 'red', linewidth=2, label='Prediction')\n",
        "#   axs[data_id, 1].set_aspect('equal')\n",
        "\n",
        "#   axs[data_id, 2].plot(entropy, label='Entropy')\n",
        "#   axs[data_id, 2].set_ylim(-8.0, -5.5)\n",
        "\n",
        "#   axs[data_id, 3].plot(SEs, label='Square Error')\n",
        "#   axs[data_id, 3].set_ylim(0.0, 0.0004)\n",
        "\n",
        "# axs[0,0].legend()\n",
        "# axs[0,1].legend()\n",
        "# axs[0,2].legend()\n",
        "# axs[0,3].legend()\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "entropy_data = np.array(entropy_list_baseline, dtype=object)\n",
        "np.save(root_dir+\"entropy_data/permutation_baseline_entropy.npy\", entropy_data)"
      ],
      "metadata": {
        "id": "6nChJ6bAA-yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All-In"
      ],
      "metadata": {
        "id": "ejkHq30k-TkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "LaAqpLGd-TkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "all_in_set = MyDataset(data, var_names=[\"A\"], permutation=True)\n",
        "\n",
        "#Use 90% of data for training\n",
        "num_train = int(len(all_in_set)*0.9)\n",
        "\n",
        "#Randomly split data into train/val sets\n",
        "torch.manual_seed(0) #Set torch's random seed so that random split of data is reproducible\n",
        "train_set, val_set = random_split(all_in_set, [num_train, len(all_in_set)-num_train])\n",
        "\n",
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(val_set, batch_size=batch_size)\n",
        "\n",
        "print('Number of training data:', len(train_set))\n",
        "print('Number of validation data:', len(val_set))\n",
        "test_x, test_y = next(iter(train_loader))\n",
        "print('x shape:', test_x.shape, '\\ny shape:', test_y.shape)"
      ],
      "metadata": {
        "outputId": "7f231167-3a07-4f1b-c440-7e956b5dbe45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vybDsDv5-TkS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training data: 34614\n",
            "Number of validation data: 3847\n",
            "x shape: torch.Size([256, 96]) \n",
            "y shape: torch.Size([256, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 4000\n",
        "LEARNING_RATE = 1e-2\n",
        "best_vloss = 1_000_000\n",
        "\n",
        "if not os.path.isdir(root_dir + \"Training/runs/\"):\n",
        "  os.makedirs(root_dir + \"Training/runs/\")\n",
        "if not os.path.isdir(root_dir + \"Training/models/\"):\n",
        "  os.makedirs(root_dir + \"Training/models/\")\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "writer = SummaryWriter(root_dir + 'Training/runs/permutation_all_in_{}_epochs_{}_lr_{}'.format(timestamp, EPOCHS, LEARNING_RATE))\n",
        "epoch_number = 0\n",
        "\n",
        "input_dim, output_dim = all_in_set.in_out_dim()\n",
        "model = LinearModel(input_dim=input_dim,output_dim=output_dim)\n",
        "if torch.cuda.is_available():\n",
        "  model = model.to(device)\n",
        "  print(\"Using GPU\")\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "visual_loss_fn = torch.nn.MSELoss()\n",
        "print(timestamp)"
      ],
      "metadata": {
        "id": "ksuiYZDV-TkT",
        "outputId": "6f439bfb-44eb-4f8c-bfe1-47aaa13567a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n",
            "20221024042212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=/content/drive/Shareddrives/Intersection_following_data/Training/runs"
      ],
      "metadata": {
        "id": "mU6SvS9w-TkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_avg_loss = 1e20\n",
        "min_avg_visual_loss = 1e20\n",
        "min_avg_vloss = 1e20\n",
        "min_avg_visual_vloss = 1e20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    # Training\n",
        "    model.train(True)\n",
        "    avg_loss, avg_visual_loss = train_one_epoch(epoch_number, gpu=torch.cuda.is_available())\n",
        "    if avg_loss < min_avg_loss:\n",
        "      min_avg_loss = avg_loss\n",
        "    if avg_visual_loss < min_avg_visual_loss:\n",
        "      min_avg_visual_loss = avg_visual_loss\n",
        "\n",
        "    # Validation\n",
        "    model.train(False)\n",
        "    running_vloss = 0.0\n",
        "    running_visual_vloss = 0.0\n",
        "    running_vacc = 0.0\n",
        "    with torch.no_grad():\n",
        "      for i, vdata in enumerate(valid_loader):\n",
        "          vinputs, vlabels = vdata\n",
        "          vinputs = torch.unsqueeze(vinputs, dim=1).float()\n",
        "          vlabels = vlabels.float()\n",
        "          if torch.cuda.is_available():\n",
        "            voutputs = model(vinputs.to(device))\n",
        "            voutputs.loc = voutputs.loc.cpu()\n",
        "            voutputs.covariance_matrix = voutputs.covariance_matrix.cpu()\n",
        "          else:\n",
        "            voutputs = model(vinputs)\n",
        "          vloss = loss_fn(voutputs, vlabels)\n",
        "          visual_vloss = visual_loss_fn(torch.squeeze(voutputs.loc), vlabels)\n",
        "          running_vloss += vloss\n",
        "          running_visual_vloss += visual_vloss\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    avg_visual_vloss = running_visual_vloss / (i + 1)\n",
        "    if avg_vloss < min_avg_vloss:\n",
        "      min_avg_vloss = avg_vloss\n",
        "    if avg_visual_vloss < min_avg_visual_vloss:\n",
        "      min_avg_visual_vloss = avg_visual_vloss\n",
        "\n",
        "    # Print loss\n",
        "    if epoch_number % int(EPOCHS/10) == int(EPOCHS/10)-1:\n",
        "      print('EPOCH {}: LOSS train {} min. train {} valid {}'.format(epoch_number+1, avg_loss, min_avg_loss, avg_vloss))\n",
        "    \n",
        "    # Log loss\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss},\n",
        "                    epoch_number + 1)\n",
        "    writer.add_scalars('Training vs. Validation MSE Loss',\n",
        "                    { 'Training' : avg_visual_loss, 'Validation' : avg_visual_vloss},\n",
        "                    epoch_number + 1)\n",
        "    writer.add_scalars('Training vs. Validation Min Loss',\n",
        "                    { 'Training' : min_avg_loss, 'Validation' : min_avg_vloss},\n",
        "                    epoch_number + 1)\n",
        "    writer.add_scalars('Training vs. Validation Min MSE Loss',\n",
        "                    { 'Training' : min_avg_visual_loss, 'Validation' : min_avg_visual_vloss},\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = root_dir + 'Training/models/permutation_best_all_in_{}'.format(timestamp)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1"
      ],
      "metadata": {
        "id": "w7VWcqMb-TkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "xkwULj4O-TkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim, output_dim = all_in_set.in_out_dim()\n",
        "# timestamp = \"20221024042212\"\n",
        "best_model = LinearModel(input_dim=input_dim,output_dim=output_dim)\n",
        "best_model.load_state_dict(torch.load(root_dir + 'Training/models/permutation_best_all_in_{}'.format(timestamp)))"
      ],
      "metadata": {
        "outputId": "e7f0c28b-d3c2-48b3-ff9e-fffbbea9769a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO3tbh_U-TkU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_names, out_names = all_in_set.get_idxs()\n",
        "\n",
        "row = 49\n",
        "col = 4\n",
        "# fig, axs = plt.subplots(row,col, figsize=(col*5,row*5))\n",
        "\n",
        "entropy_list_all_in = []\n",
        "\n",
        "for data_id in range(row):\n",
        "\n",
        "  test_x = torch.from_numpy((data[data_id].to_numpy()[:,in_names])).float()\n",
        "  test_y = torch.from_numpy((data[data_id].to_numpy()[:,out_names])).float()\n",
        "  # test_x = torch.unsqueeze(test_x, dim=1).float()\n",
        "  pred_y = best_model(test_x)\n",
        "\n",
        "  test_y = torch.squeeze(test_y).detach().numpy()\n",
        "  pred_y.loc = torch.squeeze(pred_y.loc)\n",
        "  pred_y.covariance_matrix = torch.squeeze(pred_y.covariance_matrix)\n",
        "  pred_y_loc = pred_y.loc.detach().numpy()\n",
        "\n",
        "  # SEs = np.square(np.linalg.norm((test_y-pred_y_loc), axis=1))\n",
        "\n",
        "  # samples = []\n",
        "  # for i in range(pred_y.loc.shape[0]):\n",
        "  #   multinorm = MultivariateNormal(loc=pred_y.loc[i], covariance_matrix=pred_y.covariance_matrix[i])\n",
        "  #   for j in range(10):\n",
        "  #     samples.append(multinorm.sample().detach().numpy())\n",
        "  # samples = np.stack(samples)\n",
        "\n",
        "  entropy = []\n",
        "  for i in range(pred_y.loc.shape[0]):\n",
        "    multinorm = MultivariateNormal(loc=pred_y.loc[i], covariance_matrix=pred_y.covariance_matrix[i])\n",
        "    entropy.append(multinorm.entropy().detach().numpy())\n",
        "  entropy = np.stack(entropy)\n",
        "  entropy_list_all_in.append(entropy)\n",
        "\n",
        "#   axs[data_id, 0].plot(test_y[:,0], test_y[:,1], 'blue', linewidth=2, label='True trajectory')\n",
        "#   axs[data_id, 0].plot(pred_y_loc[:,0], pred_y_loc[:,1], 'red', linewidth=2, label='Prediction')\n",
        "#   axs[data_id, 0].set_aspect('equal')\n",
        "\n",
        "#   axs[data_id, 1].scatter(samples[:,0], samples[:,1], s=5, c='gray', alpha=0.2, label='Samples')\n",
        "#   axs[data_id, 1].plot(test_y[:,0], test_y[:,1], 'blue', linewidth=2, label='True trajectory')\n",
        "#   axs[data_id, 1].plot(pred_y_loc[:,0], pred_y_loc[:,1], 'red', linewidth=2, label='Prediction')\n",
        "#   axs[data_id, 1].set_aspect('equal')\n",
        "\n",
        "#   axs[data_id, 2].plot(entropy, label='Entropy')\n",
        "#   axs[data_id, 2].set_ylim(-7.0, -5.0)\n",
        "\n",
        "#   axs[data_id, 3].plot(SEs, label='Square Error')\n",
        "\n",
        "# axs[0,0].legend()\n",
        "# axs[0,1].legend()\n",
        "# axs[0,2].legend()\n",
        "# axs[0,3].legend()\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "entropy_data = np.array(entropy_list_all_in, dtype=object)\n",
        "np.save(root_dir+\"entropy_data/permutation_all_in_entropy.npy\", entropy_data)"
      ],
      "metadata": {
        "id": "ypMXUghB-TkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Validation"
      ],
      "metadata": {
        "id": "BMLIYJ4m2DUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "HEV9Khdx2DUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "var_name = \"head_Av\"\n",
        "model_name = var_name+\"xy\"\n",
        "print(model_name)\n",
        "CV_set = MyDataset(data, var_names=[var_name+\"x\",var_name+\"y\"], permutation=True)\n",
        "\n",
        "#Use 90% of data for training\n",
        "num_train = int(len(CV_set)*0.9)\n",
        "\n",
        "#Split data into train/val sets\n",
        "torch.manual_seed(0) #Set torch's random seed so that random split of data is reproducible\n",
        "train_set, val_set = random_split(CV_set, [num_train, len(CV_set)-num_train])\n",
        "\n",
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(val_set, batch_size=batch_size)\n",
        "\n",
        "print('Number of training data:', len(train_set))\n",
        "print('Number of validation data:', len(val_set))\n",
        "test_x, test_y = next(iter(train_loader))\n",
        "print('x shape:', test_x.shape, '\\ny shape:', test_y.shape)"
      ],
      "metadata": {
        "outputId": "64c7c67c-00a1-4a2a-8439-db2be666e4a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3G9v-_F2DUZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "head_Avxy\n",
            "Number of training data: 34614\n",
            "Number of validation data: 3847\n",
            "x shape: torch.Size([256, 56]) \n",
            "y shape: torch.Size([256, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 4000\n",
        "LEARNING_RATE = 1e-2\n",
        "best_vloss = 1_000_000\n",
        "\n",
        "if not os.path.isdir(root_dir + \"Training/runs/\"):\n",
        "  os.makedirs(root_dir + \"Training/runs/\")\n",
        "if not os.path.isdir(root_dir + \"Training/models/\"):\n",
        "  os.makedirs(root_dir + \"Training/models/\")\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "writer = SummaryWriter(root_dir + 'Training/runs/permutation_{}_{}_epochs_{}_lr_{}'.format(model_name, timestamp, EPOCHS, LEARNING_RATE))\n",
        "epoch_number = 0\n",
        "\n",
        "input_dim, output_dim = CV_set.in_out_dim()\n",
        "model = LinearModel(input_dim=input_dim,output_dim=output_dim)\n",
        "if torch.cuda.is_available():\n",
        "  model = model.to(device)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "visual_loss_fn = torch.nn.MSELoss()\n",
        "print(timestamp)"
      ],
      "metadata": {
        "id": "P3H1sHkI2DUa",
        "outputId": "67ba78f0-4054-4ab9-d6ab-eaa51627866d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20221025062411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=/content/drive/Shareddrives/Intersection_following_data/Training/runs"
      ],
      "metadata": {
        "id": "VEf1qc6c2DUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_avg_loss = 1e20\n",
        "min_avg_visual_loss = 1e20\n",
        "min_avg_vloss = 1e20\n",
        "min_avg_visual_vloss = 1e20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    # Training\n",
        "    model.train(True)\n",
        "    avg_loss, avg_visual_loss = train_one_epoch(epoch_number, gpu=torch.cuda.is_available())\n",
        "    if avg_loss < min_avg_loss:\n",
        "      min_avg_loss = avg_loss\n",
        "    if avg_visual_loss < min_avg_visual_loss:\n",
        "      min_avg_visual_loss = avg_visual_loss\n",
        "\n",
        "    # Validation\n",
        "    model.train(False)\n",
        "    running_vloss = 0.0\n",
        "    running_visual_vloss = 0.0\n",
        "    running_vacc = 0.0\n",
        "    with torch.no_grad():\n",
        "      for i, vdata in enumerate(valid_loader):\n",
        "          vinputs, vlabels = vdata\n",
        "          vinputs = torch.unsqueeze(vinputs, dim=1).float()\n",
        "          vlabels = vlabels.float()\n",
        "          if torch.cuda.is_available():\n",
        "            voutputs = model(vinputs.to(device))\n",
        "            voutputs.loc = voutputs.loc.cpu()\n",
        "            voutputs.covariance_matrix = voutputs.covariance_matrix.cpu()\n",
        "          else:\n",
        "            voutputs = model(vinputs)\n",
        "          vloss = loss_fn(voutputs, vlabels)\n",
        "          visual_vloss = visual_loss_fn(torch.squeeze(voutputs.loc), vlabels)\n",
        "          running_vloss += vloss\n",
        "          running_visual_vloss += visual_vloss\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    avg_visual_vloss = running_visual_vloss / (i + 1)\n",
        "    if avg_vloss < min_avg_vloss:\n",
        "      min_avg_vloss = avg_vloss\n",
        "    if avg_visual_vloss < min_avg_visual_vloss:\n",
        "      min_avg_visual_vloss = avg_visual_vloss\n",
        "\n",
        "    # Print loss\n",
        "    if epoch_number % int(EPOCHS/10) == int(EPOCHS/10)-1:\n",
        "      print('EPOCH {}: LOSS train {} min. train {} valid {}'.format(epoch_number+1, avg_loss, min_avg_loss, avg_vloss))\n",
        "    \n",
        "    # Log loss\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss},\n",
        "                    epoch_number + 1)\n",
        "    writer.add_scalars('Training vs. Validation MSE Loss',\n",
        "                    { 'Training' : avg_visual_loss, 'Validation' : avg_visual_vloss},\n",
        "                    epoch_number + 1)\n",
        "    writer.add_scalars('Training vs. Validation Min Loss',\n",
        "                    { 'Training' : min_avg_loss, 'Validation' : min_avg_vloss},\n",
        "                    epoch_number + 1)\n",
        "    writer.add_scalars('Training vs. Validation Min MSE Loss',\n",
        "                    { 'Training' : min_avg_visual_loss, 'Validation' : min_avg_visual_vloss},\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = root_dir + 'Training/models/permutation_best_{}_{}'.format(model_name, timestamp)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1"
      ],
      "metadata": {
        "id": "RvkEjgEM2DUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "Cdt5CGDh2DUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim, output_dim = CV_set.in_out_dim()\n",
        "# timestamp = \"20221025062411\"\n",
        "best_model = LinearModel(input_dim=input_dim,output_dim=output_dim)\n",
        "best_model.load_state_dict(torch.load(root_dir + 'Training/models/permutation_best_{}_{}'.format(model_name, timestamp), map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "outputId": "ce8ca2fd-482a-4bd3-b720-f30a762d7304",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duC8uCzN2DUb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_names, out_names = CV_set.get_idxs()\n",
        "\n",
        "row = 49\n",
        "col = 4\n",
        "# fig, axs = plt.subplots(row,col, figsize=(col*5,row*5))\n",
        "\n",
        "entropy_list_cv = []\n",
        "\n",
        "for data_id in range(row):\n",
        "\n",
        "  test_x = torch.from_numpy((data[data_id].to_numpy()[:,in_names])).float()\n",
        "  test_y = torch.from_numpy((data[data_id].to_numpy()[:,out_names])).float()\n",
        "  # test_x = torch.unsqueeze(test_x, dim=1).float()\n",
        "  pred_y = best_model(test_x)\n",
        "\n",
        "  test_y = torch.squeeze(test_y).detach().numpy()\n",
        "  pred_y.loc = torch.squeeze(pred_y.loc)\n",
        "  pred_y.covariance_matrix = torch.squeeze(pred_y.covariance_matrix)\n",
        "  pred_y_loc = pred_y.loc.detach().numpy()\n",
        "\n",
        "  # SEs = np.square(np.linalg.norm((test_y-pred_y_loc), axis=1))\n",
        "\n",
        "  # samples = []\n",
        "  # for i in range(pred_y.loc.shape[0]):\n",
        "  #   multinorm = MultivariateNormal(loc=pred_y.loc[i], covariance_matrix=pred_y.covariance_matrix[i])\n",
        "  #   for j in range(10):\n",
        "  #     samples.append(multinorm.sample().detach().numpy())\n",
        "  # samples = np.stack(samples)\n",
        "\n",
        "  entropy = []\n",
        "  for i in range(pred_y.loc.shape[0]):\n",
        "    multinorm = MultivariateNormal(loc=pred_y.loc[i], covariance_matrix=pred_y.covariance_matrix[i])\n",
        "    entropy.append(multinorm.entropy().detach().numpy())\n",
        "  entropy = np.stack(entropy)\n",
        "  entropy_list_cv.append(entropy)\n",
        "\n",
        "#   axs[data_id, 0].plot(test_y[:,0], test_y[:,1], 'blue', linewidth=2, label='True trajectory')\n",
        "#   axs[data_id, 0].plot(pred_y_loc[:,0], pred_y_loc[:,1], 'red', linewidth=2, label='Prediction')\n",
        "#   axs[data_id, 0].set_aspect('equal')\n",
        "\n",
        "#   axs[data_id, 1].scatter(samples[:,0], samples[:,1], s=5, c='gray', alpha=0.2, label='Samples')\n",
        "#   axs[data_id, 1].plot(test_y[:,0], test_y[:,1], 'blue', linewidth=2, label='True trajectory')\n",
        "#   axs[data_id, 1].plot(pred_y_loc[:,0], pred_y_loc[:,1], 'red', linewidth=2, label='Prediction')\n",
        "#   axs[data_id, 1].set_aspect('equal')\n",
        "\n",
        "#   axs[data_id, 2].plot(entropy, label='Entropy')\n",
        "\n",
        "#   axs[data_id, 3].plot(SEs, label='Square Error (mm\\u00b2)')\n",
        "\n",
        "# axs[0,0].legend()\n",
        "# axs[0,1].legend()\n",
        "# axs[0,2].legend()\n",
        "# axs[0,3].legend()\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "entropy_data = np.array(entropy_list_cv, dtype=object)\n",
        "np.save(root_dir+\"entropy_data/permutation_cv_{}_entropy.npy\".format(var_name), entropy_data)"
      ],
      "metadata": {
        "id": "eLCcYMcP2DUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VYNXp9KFeD-2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}