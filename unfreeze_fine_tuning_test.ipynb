{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyTBgS09s3Eth/XFAcXRu3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhy9968/ECE6179_project/blob/main/unfreeze_fine_tuning_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFBpiWvvDE4N",
        "outputId": "24e6e115-e2d1-424d-92d1-0a2dd08e9b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 36.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.9.3\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision.datasets import STL10 as STL10\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split\n",
        "!pip install torchmetrics\n",
        "from torchmetrics import Accuracy\n",
        "import torchvision\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "####### CHANGE TO APPROPRIATE DIRECTORY TO STORE DATASET\n",
        "root_dir = \"/content/drive/Shareddrives/ECE6179_project/\"\n",
        "dataset_dir = root_dir + \"CNN-VAE/data/\"\n",
        "#For MonARCH\n",
        "# dataset_dir = \"/mnt/lustre/projects/ds19/SHARED\"\n",
        "\n",
        "#All images are 3x96x96\n",
        "image_size = 96\n",
        "#Example batch size\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wXg8yiPDE4Q"
      },
      "outputs": [],
      "source": [
        "#Perform random crops and mirroring for data augmentation\n",
        "transform_train = transforms.Compose(\n",
        "    [transforms.RandomCrop(image_size, padding=4),\n",
        "     transforms.RandomHorizontalFlip(p=0.5),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform_unlabelled = transforms.Compose(\n",
        "    [transforms.RandomHorizontalFlip(p=0.5),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "#No random \n",
        "transform_test = transforms.Compose(\n",
        "    [transforms.CenterCrop(image_size),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rgiNAlXDE4T",
        "outputId": "cd91968a-2dba-46bd-8a8c-cf1653899ce7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#Load train and validation sets\n",
        "trainval_set = STL10(dataset_dir, split='train', transform=transform_train, download=True)\n",
        "\n",
        "#Use 10% of data for training - simulating low data scenario\n",
        "num_train = int(len(trainval_set)*0.1)\n",
        "\n",
        "#Split data into train/val sets\n",
        "torch.manual_seed(0) #Set torch's random seed so that random split of data is reproducible\n",
        "train_set, val_set = random_split(trainval_set, [num_train, len(trainval_set)-num_train])\n",
        "\n",
        "#Load test set\n",
        "test_set = STL10(dataset_dir, split='test', transform=transform_test, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DouBlqObDE4V",
        "outputId": "1117b278-f34f-4330-8a87-1e25d80359e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "unlabelled_set = STL10(dataset_dir, split='unlabeled', transform=transform_unlabelled, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVrmhgwmDE4X",
        "outputId": "6ce455aa-e51c-4166-cd96-ca59f933bcc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of train loader: 10\n",
            "Size of valid loader: 90\n",
            "Size of test loader: 160\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 50\n",
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
        "# unlabelled_loader = DataLoader(unlabelled_set, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size)\n",
        "test_loader  = DataLoader(test_set, batch_size=batch_size)\n",
        "\n",
        "print('Size of train loader:', len(train_loader))\n",
        "# print('Size of unlabelled loader:', len(unlabelled_loader))\n",
        "print('Size of valid loader:', len(val_loader))\n",
        "print('Size of test loader:', len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_iter = iter(unlabelled_loader)\n",
        "data,labels = train_data_iter.next()\n",
        "print(data.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "id": "vzHG6jOZJhFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "VRD-xfRzlbSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --quiet \"matplotlib\" \"pytorch-lightning\" \"pandas\" \"torchmetrics\"\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torchmetrics import Accuracy\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from torch.nn import functional as F\n",
        "from torchmetrics import MeanSquaredError\n",
        "from pytorch_lightning.callbacks import Callback, ModelCheckpoint\n",
        "import torchvision.models.resnet as RN\n",
        "\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.models.resnet import ResNet18_Weights"
      ],
      "metadata": {
        "id": "lSgdx6lWlb6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__ (self, base_model=resnet18(weights=ResNet18_Weights.IMAGENET1K_V1, progress=False)):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.block1 = nn.Sequential(*list((base_model.children()))[:5])\n",
        "    self.block2 = nn.Sequential(*list((base_model.children()))[5])\n",
        "    self.block3 = nn.Sequential(*list((base_model.children()))[6])\n",
        "    self.block4 = nn.Sequential(*list((base_model.children()))[7])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.block1(x)\n",
        "    x = self.block2(x)\n",
        "    x = self.block3(x)\n",
        "    x = self.block4(x)\n",
        "    return x\n",
        "\n",
        "  def print_model(self):\n",
        "    print(self)\n",
        "\n",
        "  def freeze_param(self, block):\n",
        "    for i, child in enumerate(self.children()):\n",
        "      print(i)\n",
        "      if i == block-1:\n",
        "        for param in child.parameters():\n",
        "          param.requires_grad = False\n",
        "          print(param.shape)\n",
        "    print('Freeze block '+str(block)+' parameters')\n",
        "    \n",
        "  def unfreeze_param(self, block):\n",
        "    for i, child in enumerate(self.children()):\n",
        "      if i == block-1:\n",
        "        for param in child.parameters():\n",
        "          param.requires_grad = True\n",
        "    print('Unfreeze block '+str(block)+' parameters')"
      ],
      "metadata": {
        "id": "gxFHtSaYWbAV",
        "outputId": "07043a40-756d-47e1-b0e5-827d072efbc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):  # Fixed inpplanes\n",
        "  def __init__ (self, inplanes = 512, intMed_planes = 64):   # Changed inplanes from 64 to 512 to match the new Encoder\n",
        "    super(Decoder, self).__init__()\n",
        "    self.inplanes      = inplanes\n",
        "    self.intMed_planes = intMed_planes\n",
        "\n",
        "    self.convTrans1 = nn.ConvTranspose2d(in_channels = 512, out_channels = 256, kernel_size = 3, stride = 2, padding = 1, output_padding=1)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.convTrans3 = nn.ConvTranspose2d(in_channels = 256, out_channels = 128, kernel_size = 3, stride = 2, padding = 1, output_padding=1)\n",
        "    self.conv4 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.convTrans5 = nn.ConvTranspose2d(in_channels = 128, out_channels = 64, kernel_size = 3, stride = 2, padding = 1, output_padding=1)\n",
        "    self.conv6 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)   \n",
        "    self.convTrans7 = nn.ConvTranspose2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 2, padding = 1, output_padding=1)\n",
        "    self.conv8 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.convTrans9 = nn.ConvTranspose2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 2, padding = 1, output_padding=1)\n",
        "    self.conv10 = nn.Conv2d(in_channels = 64, out_channels = 3, kernel_size = 3, stride = 1, padding = 1)   \n",
        "\n",
        " # Output padding is here to match the size. It needs to be careful on this extra line of zeros when building the loss function.\n",
        "\n",
        "  def forward (self, x):\n",
        "    \n",
        "    x = self.convTrans1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.convTrans3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.convTrans5(x)\n",
        "    x = self.conv6(x)\n",
        "    x = self.convTrans7(x)\n",
        "    x = self.conv8(x)\n",
        "    x = self.convTrans9(x)\n",
        "    x = self.conv10(x)\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "AyLshQU3wWLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(LightningModule):\n",
        "  def __init__ (self, learning_rate = 1e-4, encoder=Encoder(), decoder=Decoder(), trainDataLoader=None, valDataLoader=None, testDataLoader=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.loss_fun = nn.MSELoss()\n",
        "\n",
        "    self.Encoder = encoder\n",
        "    self.Decoder = decoder\n",
        "\n",
        "    self.train_accuracy = MeanSquaredError()\n",
        "    self.vald_accuracy = MeanSquaredError()\n",
        "    self.test_accuracy = MeanSquaredError()\n",
        "\n",
        "    self.trainDataLoader = trainDataLoader\n",
        "    self.valDataLoader = valDataLoader\n",
        "    self.testDataLoader = testDataLoader\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "#    print(x.shape)\n",
        "    out = self.Encoder(x)\n",
        "#    print(out.shape)\n",
        "    out = self.Decoder(out)\n",
        "#    print(out.shape)\n",
        "    out_flattened = out.view(x.shape[0], -1)\n",
        "#    print(out_flattened.shape)\n",
        "    return out_flattened\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    logits = self.forward(x)\n",
        "    x_flattened = x.view(x.shape[0], -1)\n",
        "\n",
        "    loss = self.loss_fun(logits, x_flattened)\n",
        "    self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
        "    #self.log(\"train_acc\", self.train_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    logits = self.forward(x)\n",
        "    x_flattened = x.view(x.shape[0], -1)\n",
        "\n",
        "    loss = self.loss_fun(logits, x_flattened)\n",
        "    self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
        "    #self.log(\"val_acc\", self.val_accuracy, prog_bar=True, on_step=False, on_epoch=True) \n",
        "  \n",
        "  def test_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    logits = self.forward(x)\n",
        "    x_flattened = x.view(x.shape[0], -1)\n",
        "\n",
        "    loss = self.loss_fun(logits, x_flattened)\n",
        "    self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
        "    #self.log(\"test_acc\", self.test_accuracy, prog_bar=True, on_step=False, on_epoch=True) \n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    return optimizer\n",
        "  \n",
        "  def train_dataloader(self):    \n",
        "    return self.trainDataLoader\n",
        "  \n",
        "  def val_dataloader(self):\n",
        "    return self.valDataLoader\n",
        "  \n",
        "  def test_dataloader(self):\n",
        "    return self.testDataLoader\n",
        "\n",
        "  def extractSubModules(self):\n",
        "    return self.Encoder, self.Decoder"
      ],
      "metadata": {
        "id": "_X44bZjh22pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Case_Dir = \"Mod_AE_v004/\"\n",
        "path = root_dir+ Case_Dir+ \"Models/\"\n",
        "enc_rec4_statdic = torch.load(path+\"20221002044719_AE_v004_enc.pth\")\n",
        "dec_rec4_statdic = torch.load(path+\"20221002044719_AE_v004_dec.pth\")\n",
        "\n",
        "encoder_train_4 = Encoder()\n",
        "encoder_train_4.load_state_dict(enc_rec4_statdic)\n",
        "\n",
        "decoder_train_4 = Decoder()\n",
        "decoder_train_4.load_state_dict(dec_rec4_statdic)\n",
        "\n",
        "encoder_train_4.freeze_param(1)\n",
        "encoder_train_4.freeze_param(2)\n",
        "# encoder_train_4.freeze_param(3)\n",
        "# encoder_train_4.freeze_param(4)\n",
        "\n",
        "\n",
        "\n",
        "class AutoEncoder_Classifier(LightningModule):\n",
        "  def __init__ (self, learning_rate = 1e-3,\n",
        "                enc = encoder_train_4, \n",
        "                trainDataLoader=train_loader, valDataLoader=val_loader, testDataLoader=test_loader):\n",
        "    super().__init__()\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.loss_fun = nn.CrossEntropyLoss()\n",
        "\n",
        "    self.Encoder = enc\n",
        "\n",
        "    self.act     = F.relu\n",
        "    self.GAP     = nn.AdaptiveAvgPool2d(8)\n",
        "    self.linear1 = nn.Linear(512*9, 256)\n",
        "    self.linear2 = nn.Linear(256, 10)\n",
        "\n",
        "\n",
        "\n",
        "    self.train_accuracy = Accuracy()\n",
        "    self.val_accuracy = Accuracy()\n",
        "    self.test_accuracy = Accuracy()\n",
        "\n",
        "    self.trainDataLoader = trainDataLoader\n",
        "    self.valDataLoader = valDataLoader\n",
        "    self.testDataLoader = testDataLoader\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.Encoder(x)\n",
        "    out = out.view(out.shape[0], -1)\n",
        "    out = self.linear1(out)\n",
        "    out = self.act(out)\n",
        "    out = self.linear2(out) \n",
        "\n",
        "    return out\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    logits = self.forward(x)\n",
        "    loss = self.loss_fun(logits, y)\n",
        "\n",
        "    preds = logits.argmax(1)\n",
        "    self.train_accuracy.update(preds, y)\n",
        "\n",
        "    self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
        "    self.log(\"train_acc\", self.train_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    logits = self.forward(x)\n",
        "    loss = self.loss_fun(logits, y)\n",
        "\n",
        "    preds = logits.argmax(1)\n",
        "    self.val_accuracy.update(preds, y)\n",
        "    self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
        "    self.log(\"val_acc\", self.val_accuracy, prog_bar=True, on_step=False, on_epoch=True) \n",
        "  \n",
        "  def test_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    logits = self.forward(x)\n",
        "    loss = self.loss_fun(logits, y)\n",
        "\n",
        "    preds = logits.argmax(1)\n",
        "    self.test_accuracy.update(preds, y)\n",
        "    self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
        "    self.log(\"test_acc\", self.test_accuracy, prog_bar=True, on_step=False, on_epoch=True) \n",
        "\n",
        "  def configure_optimizers(self):\n",
        "\n",
        "    optimizer = torch.optim.Adam(params=self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    return optimizer\n",
        "  \n",
        "  def train_dataloader(self):    \n",
        "    return self.trainDataLoader\n",
        "  \n",
        "  def val_dataloader(self):\n",
        "    return self.valDataLoader\n",
        "  \n",
        "  def test_dataloader(self):\n",
        "    return self.testDataLoader\n",
        "\n",
        "  def extractSubModules(self):\n",
        "    return self.Encoder, self.linear1, self.linear2"
      ],
      "metadata": {
        "id": "lSfklTPTl09e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Max_Epochs = 400\n",
        "Case_Dir = \"Mod_AE_v004/Classifier/\"\n",
        "checkpoint_callback = ModelCheckpoint(monitor = \"val_acc\",\n",
        "                                      dirpath = root_dir+ Case_Dir,\n",
        "                                      save_top_k=1,\n",
        "                                      mode=\"max\",\n",
        "                                      every_n_epochs=1\n",
        "                                      )\n",
        "print(root_dir+ Case_Dir)\n",
        "\n",
        "train_loader_mod = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "\n",
        "model_AE_Classifier_v002 = AutoEncoder_Classifier(learning_rate = 2e-5, trainDataLoader=train_loader, valDataLoader=val_loader, testDataLoader=test_loader)\n",
        "checkpoint_dir = root_dir+ Case_Dir+\"lightning_logs/version_6/checkpoints/epoch=58-step=590.ckpt\"\n",
        "\n",
        "trainer_AE_Classfier = Trainer(\n",
        "    accelerator=\"auto\",\n",
        "    devices = 1 if torch.cuda.is_available() else None,\n",
        "    max_epochs = Max_Epochs,\n",
        "    callbacks = [TQDMProgressBar(refresh_rate=20)],\n",
        "    logger=CSVLogger(save_dir= root_dir+ Case_Dir),\n",
        "    deterministic=False,\n",
        "    log_every_n_steps=10\n",
        ")\n",
        "\n",
        "trainer_AE_Classfier.fit(model_AE_Classifier_v002, ckpt_path=checkpoint_dir )\n",
        "\n",
        "# Evaluate Model\n",
        "trainer_AE_Classfier.test()\n",
        "\n",
        "# Save Encoder & Decoder\n",
        "\n",
        "model_AE_Classifier_v002.freeze()\n",
        "model_path = root_dir+ Case_Dir+ \"Models/\"\n",
        "timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "\n",
        "enc, lin1, lin2 = model_AE_Classifier_v002 .extractSubModules()\n",
        "torch.save(model_AE_Classifier_v002.state_dict(), model_path+timestamp+\"_AEC_v002_mod.pth\")\n",
        "torch.save(enc.state_dict(), model_path+timestamp+\"_AEC_v002_enc.pth\")\n",
        "torch.save(lin1.state_dict(), model_path+timestamp+\"_AEC_v002_lin1.pth\")\n",
        "torch.save(lin2.state_dict(), model_path+timestamp+\"_AEC_v002_lin2.pth\")"
      ],
      "metadata": {
        "id": "Q_0WGp-Ll9Os"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}